{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can press shift + enter to quickly advance through each line of a notebook. Try it now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet of Python creates an extremely simple graph and runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic model parameters as external flags.\n",
    "# this is an interesting way of setting up constants\n",
    "# syntax is flags.define_<type_of_variable>(variable_name, value, description)\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('max_steps', 12000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer('batch_size', 50, 'Batch size.  '\n",
    "                     'Must divide evenly into the dataset sizes.')\n",
    "flags.DEFINE_string('train_checkpoint', 'data', 'checkpoint saver prefix')\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 28\n",
    "CHANNELS = 1\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE * CHANNELS\n",
    "#NUMBER_OF_INPUTS=8 #now determined by direct counting\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "training_image_paths = []\n",
    "training_image_numbers = []\n",
    "labels = []\n",
    "\n",
    "image_reader = tf.WholeFileReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"start get image paths\"\"\"\n",
    "\n",
    "def get_image_paths_in_folder(folder_name):\n",
    "  image_paths = [os.path.join(folder, pic)\n",
    "      for folder, subs, pics, in os.walk(\".\")\n",
    "      for pic in pics if pic.endswith(\".png\") and folder.startswith(folder_name)]\n",
    "  return image_paths\n",
    "\n",
    "for x in xrange(NUM_CLASSES):\n",
    "  training_image_paths.append(get_image_paths_in_folder(\"./mnist_png/testing/\" + str(x)))\n",
    "\n",
    "\n",
    "#append all images to a single array\n",
    "for x in xrange(NUM_CLASSES):\n",
    "  for filename in training_image_paths[x]:\n",
    "    image = Image.open(filename)\n",
    "    image = image.resize((IMAGE_SIZE,IMAGE_SIZE))\n",
    "    train_images.append(np.array(image))\n",
    "\n",
    "#get the lengths of each into an array\n",
    "for x in xrange(NUM_CLASSES):\n",
    "  training_image_numbers.append(len(training_image_paths[x]))\n",
    "\n",
    "\"\"\"end get image paths\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start getting labels in format\n",
    "\n",
    "for x in xrange(NUM_CLASSES):\n",
    "  labels += [x]*training_image_numbers[x]\n",
    "\n",
    "train_labels = np.array(labels)\n",
    "\n",
    "\n",
    "NUMBER_OF_INPUTS = len(train_images)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_images = train_images.reshape(NUMBER_OF_INPUTS,IMAGE_PIXELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset class for feeding next batch\n",
    "class DataSet(object):\n",
    "  train_images = []\n",
    "  train_labels = []\n",
    "  batch_index = 0\n",
    "  batch_size = 1\n",
    "  last_index = 0\n",
    "\n",
    "  def __init__(self, train_images, train_labels, batch_size):\n",
    "    self.train_images = train_images\n",
    "    self.train_labels = train_labels\n",
    "    self.batch_size = batch_size\n",
    "    self.batch_index = 0\n",
    "    self.last_index = len(train_labels) - 1\n",
    "\n",
    "  def next_batch(self):\n",
    "    temp_batch_index = self.batch_index\n",
    "    start_index = temp_batch_index * self.batch_size\n",
    "    #end index is one batch ahead of start\n",
    "    end_index = (temp_batch_index + 1 ) * self.batch_size\n",
    "\n",
    "    # if within bounds, send over the labels and images\n",
    "    if end_index <= self.last_index:\n",
    "      self.batch_index += 1\n",
    "      return self.train_images[start_index:end_index], self.train_labels[start_index:end_index]\n",
    "    # else if out of bounds, then wrap around\n",
    "    # TODO is this where we mark an epoch?\n",
    "    else:\n",
    "      self.batch_index = 0\n",
    "      temp_batch_index = self.batch_index\n",
    "      start_index = self.batch_index * self.batch_size\n",
    "      end_index = (self.batch_index + 1 ) * self.batch_size\n",
    "      return self.train_images[start_index:end_index], self.train_labels[start_index:end_index]\n",
    "\n",
    "#create data_set object similar to way in which MNIST example was created\n",
    "data_set = DataSet(train_images, train_labels, FLAGS.batch_size)\n",
    "\n",
    "\n",
    "test_images, test_labels = data_set.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "\n",
    "  # Convolutional - 1\n",
    "  with tf.name_scope('h_conv_1'):\n",
    "    W_conv_1 = weight_variable([5,5,1,32])\n",
    "    b_conv_1 = bias_variable([32])\n",
    "\n",
    "    x_image = tf.reshape(images, [-1,28,28, 1])\n",
    "    h_conv_1 = tf.nn.relu(conv2d(x_image, W_conv_1) + b_conv_1)\n",
    "\n",
    "    # Max pool 2x2 - 1\n",
    "    h_pool_1 = max_pool_2x2(h_conv_1)\n",
    "  # Convolutional - 2\n",
    "  with tf.name_scope('h_conv_2'):\n",
    "    W_conv_2 = weight_variable([5,5,32,64])\n",
    "    b_conv_2 = bias_variable([64])\n",
    "\n",
    "    h_conv_2 = tf.nn.relu(conv2d(h_pool_1, W_conv_2) + b_conv_2)\n",
    "\n",
    "    # Max pool 2x2 - 1\n",
    "    h_pool_2 = max_pool_2x2(h_conv_2)\n",
    "\n",
    "  # Densely connected 1\n",
    "  with tf.name_scope('densely_connected_1'):\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool_2_flat = tf.reshape(h_pool_2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool_2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    #keep_prob = tf.constant(0.2, dtype=tf.float32)\n",
    "    #h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "  # Readout Layer\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_loss(logits, labels):\n",
    "  labels = tf.to_int64(labels)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "  loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  return loss\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "def placeholder_inputs(batch_size):\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=[batch_size,IMAGE_PIXELS])\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  return images_placeholder, labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "\n",
    "  images_feed, labels_feed = data_set.next_batch()\n",
    "\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder, labels_placeholder,\n",
    "            data_set):\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = NUMBER_OF_INPUTS // FLAGS.batch_size\n",
    "  num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = true_count / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @1.00: %0.04f' %\n",
    "        (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "  # Tell TensorFlow that the model will be built into the default Graph.\n",
    "  with tf.Graph().as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size)\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(images_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss = cal_loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = training(loss, FLAGS.learning_rate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    # And then after everything is built, start the training loop.\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "\n",
    "      feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "\n",
    "      _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      if step % 2 == 0:\n",
    "        # Print status to stdout.\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        saver.save(sess, FLAGS.train_checkpoint, global_step=step)\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e98b7658332b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0mtraining_image_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_image_paths_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./mnist_png/testing/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e98b7658332b>\u001b[0m in \u001b[0;36mget_image_paths_in_folder\u001b[0;34m(folder_name)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_image_paths_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   image_paths = [os.path.join(folder, pic)\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m       for pic in pics if pic.endswith(\".png\") and folder.startswith(folder_name)]\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see \"Hello, TensorFlow!\", everything is working well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next to [1_linear_regression_model.ipynb](1_linear_regression_model.ipynb).</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
